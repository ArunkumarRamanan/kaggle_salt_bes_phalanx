{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import gc\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "# from utils import predict_test, noise_cv, evaluate, ensemble, get_model, ThreadsafeIter\n",
    "# from models import *\n",
    "# # from iterators import DataGenerator\n",
    "# from generators import DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Validation and train preprocessing\n",
    "# noise_cv()\n",
    "# df_train = pd.read_csv(os.path.join(DATA_ROOT,'train.csv'),index_col='id')\n",
    "# depths = pd.read_csv(os.path.join(DATA_ROOT,'depths.csv'),index_col='id')\n",
    "# folds = pd.read_csv(os.path.join(DATA_ROOT,'folds.csv'),index_col='id')\n",
    "# df_train.join(depths).join(folds).sample(frac=1,random_state=123).to_csv(os.path.join(DATA_ROOT,'train_proc.csv'))\n",
    "\n",
    "# dist = []\n",
    "# for id in train.id.values:\n",
    "#     img = cv2.imread('train/images/{}.png'.format(id), cv2.IMREAD_GRAYSCALE)\n",
    "#     dist.append(np.unique(img).shape[0])\n",
    "# train['unique_pixels'] = dist\n",
    "# train.to_csv('data/train_proc_v2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = 'data/'\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 300\n",
    "BASE_PATH = '/home/branding_images/salt/'\n",
    "MODEL_NAME = 'resnet_50_224'\n",
    "MODEL_PATH = os.path.join(BASE_PATH,MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(DATA_ROOT,'train_proc_v2.csv'))\n",
    "test = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import args\n",
    "from models.models import get_model\n",
    "from utils import freeze_model\n",
    "from datasets.salt import SaltDataset\n",
    "from augmentations import get_augmentations\n",
    "from callbacks import get_callback\n",
    "from utils import ThreadsafeIter\n",
    "from losses import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import unets\n",
    "unets.resnet50_fpn((96,96,3),1).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = [int(f) for f in args.fold.split(',')]\n",
    "for fold in folds:\n",
    "    K.clear_session()\n",
    "    \n",
    "    print('***************************** FOLD {} *****************************'.format(fold))\n",
    "    \n",
    "    model = get_model(args.network, (args.input_size, args.input_size, 3))\n",
    "    # model.summary()\n",
    "    \n",
    "    MODEL_PATH = os.path.join(args.models_dir,args.network)\n",
    "    if fold == 0:\n",
    "        if os.path.isdir(MODEL_PATH):\n",
    "            raise ValueError('Such Model already exists')\n",
    "        os.system(\"mkdir {}\".format(MODEL_PATH))\n",
    "    \n",
    "    if args.weights is None:\n",
    "        print('No weights passed, training from scratch')\n",
    "    else:\n",
    "        weights_path = args.weights.format(fold)\n",
    "        print('Loading weights from {}'.format(weights_path))\n",
    "        model.load_weights(weights_path, by_name=True)\n",
    "    \n",
    "    # Check with model summary!\n",
    "    freeze_model(model, args.freeze_till_layer)\n",
    "    # model.summary()\n",
    "    \n",
    "    optimizer = RMSprop(lr=args.learning_rate)\n",
    "    if args.optimizer:\n",
    "        if args.optimizer == 'rmsprop':\n",
    "            optimizer = RMSprop(lr=args.learning_rate, decay=float(args.decay))\n",
    "        elif args.optimizer == 'adam':\n",
    "            optimizer = Adam(lr=args.learning_rate, decay=float(args.decay))\n",
    "    \n",
    "    dataset = SaltDataset(args.images_dir, args.masks_dir, fold, args.n_folds, seed=args.seed)\n",
    "    augmentations = get_augmentations(args.augmentation_name,p=args.augmentation_prob)\n",
    "        \n",
    "    train_generator = dataset.train_generator((args.input_size, args.input_size),\n",
    "                                              args.batch_size,\n",
    "                                              args.preprocessing_function,\n",
    "                                              augmentations)\n",
    "    \n",
    "    val_generator = dataset.val_generator((args.input_size, args.input_size),\n",
    "                                          args.batch_size * 2,\n",
    "                                          args.preprocessing_function)\n",
    "    \n",
    "    \n",
    "    best_model_file = os.path.join(MODEL_PATH,'best_{}{}_fold{}.h5'.format(args.alias, args.network, fold))\n",
    "    \n",
    "    callbacks = get_callback(args.callback,\n",
    "                 weights_path=best_model_file,\n",
    "                 early_stop_patience=args.early_stop_patience,\n",
    "                 reduce_lr_factor=args.reduce_lr_factor,\n",
    "                 reduce_lr_patience=args.reduce_lr_patience,\n",
    "                 reduce_lr_min=args.reduce_lr_min)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=make_loss('bce_jacard'),\n",
    "                      metrics=[dice_coef, jacard_coef])\n",
    "    \n",
    "    # Fit the model with Generators:\n",
    "    history = model.fit_generator(generator=ThreadsafeIter(train_generator),\n",
    "                    steps_per_epoch=dataset.train_ids.shape[0] // args.batch_size * 2,\n",
    "                    epochs=args.epochs,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=ThreadsafeIter(val_generator),\n",
    "                    validation_steps=np.ceil(dataset.val_ids.shape[0] / args.batch_size),\n",
    "                    workers=8)\n",
    "    \n",
    "    K.clear_session()\n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "dices = []\n",
    "epochs = []\n",
    "for fold in range(4,5):\n",
    "    from keras import backend as K\n",
    "    K.clear_session()\n",
    "    print('***************************** FOLD {} *****************************'.format(fold))\n",
    "    \n",
    "    if fold == 0:\n",
    "        if os.path.isdir(MODEL_PATH):\n",
    "            print('Such Model already exists')\n",
    "            break\n",
    "        os.system(\"mkdir {}\".format(MODEL_PATH))\n",
    "    \n",
    "    df_train = train[train.fold != fold].copy().reset_index(drop=True)\n",
    "    df_valid = train[train.fold == fold].copy().reset_index(drop=True)\n",
    "    \n",
    "    ids_train, ids_valid = df_train[df_train.unique_pixels>1].id.values, df_valid.id.values\n",
    "    \n",
    "    print('Training on {} samples'.format(ids_train.shape[0]))\n",
    "    print('Validating on {} samples'.format(ids_valid.shape[0]))\n",
    "    \n",
    "    # Initialize Model\n",
    "    weights_path = os.path.join(MODEL_PATH,'fold_{fold}.hdf5'.format(fold=fold))\n",
    "    model, callbacks, input_size, augs, preprocess = get_model(weights_path, MODEL_NAME)\n",
    "\n",
    "#     # Fit the model with Iterators\n",
    "#     training_generator = DataGenerator(ids_train, is_train=True, **params)\n",
    "#     valid_generator = DataGenerator(ids_valid, is_train=False, **params)\n",
    "#     # Train model on dataset\n",
    "#     history = model.fit_generator(generator=training_generator,\n",
    "#                         validation_data=valid_generator,\n",
    "#                         epochs=EPOCHS,\n",
    "#                         callbacks=callbacks,\n",
    "#                         use_multiprocessing=False,\n",
    "#                         workers=4)\n",
    "\n",
    "    dg = DataGenerator(input_size=input_size[0], n_channels=input_size[-1], batch_size = BATCH_SIZE, augs = augs,\n",
    "                      preprocess = preprocess)\n",
    "    train_generator = dg.train_batch_generator(ids_train)\n",
    "    validation_generator = dg.evaluation_batch_generator(ids_valid)\n",
    "    \n",
    "    # Fit the model with Generators:\n",
    "    history = model.fit_generator(generator=ThreadsafeIter(train_generator),\n",
    "                    steps_per_epoch=ids_train.shape[0] // BATCH_SIZE * 2,\n",
    "                    epochs=EPOCHS,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=ThreadsafeIter(validation_generator),\n",
    "                    validation_steps=np.ceil(ids_valid.shape[0] / BATCH_SIZE),\n",
    "                    workers=8)\n",
    "    \n",
    "    best_epoch = np.argmin(history.history['val_loss'])\n",
    "    best_val_loss = np.min(history.history['val_loss'])\n",
    "    best_val_dice_coeff = history.history['val_dice_coef'][best_epoch]\n",
    "\n",
    "    losses.append(best_val_loss)\n",
    "    dices.append(best_val_dice_coeff)\n",
    "    epochs.append(best_epoch)\n",
    "    print(best_val_loss)\n",
    "         \n",
    "    model.load_weights(weights_path)\n",
    "\n",
    "    # SAVE OOF PREDICTIONS\n",
    "    dir_path = os.path.join(MODEL_PATH,'oof')\n",
    "    os.system(\"mkdir {}\".format(dir_path))\n",
    "    pred = predict_test(model=model,\n",
    "                    preds_path=dir_path,\n",
    "                    oof=True,\n",
    "                    ids=ids_valid,\n",
    "                    batch_size=BATCH_SIZE*4,\n",
    "                    thr=0.5,\n",
    "                    TTA='',\n",
    "                    preprocess=preprocess)\n",
    "    \n",
    "    # SAVE TEST PREDICTIONS\n",
    "    dir_path = os.path.join(MODEL_PATH,'fold_{}'.format(fold))\n",
    "    os.system(\"mkdir {}\".format(dir_path))\n",
    "    pred = predict_test(model=model,\n",
    "                    preds_path=dir_path,\n",
    "                    oof=False,\n",
    "                    ids=test.id.values,\n",
    "                    batch_size=BATCH_SIZE*4,\n",
    "                    thr=0.5,\n",
    "                    TTA='',\n",
    "                    preprocess=preprocess)\n",
    "    \n",
    "    # Run a single fold\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Single Batch\n",
    "val_loss: 0.2542 - val_dice_coef: 0.8197 - val_jacard_coef: 0.6983\n",
    "iout: 0.679506\n",
    "jaccard: 0.714482\n",
    "    \n",
    "Double Batch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([0.8616627585740737,\n",
    " 0.8088357706568134,\n",
    " 0.8030663774281042,\n",
    " 0.835186310659481,\n",
    " 0.8300570451529922])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH2 = '/home/branding_images/salt/initial_generator/'\n",
    "# MODEL_PATH2 = '/home/branding_images/salt/unet_128/'\n",
    "# MODEL_PATH2 = '/home/branding_images/salt/unet_128_dropout_adam/'\n",
    "# MODEL_PATH2 = '/home/branding_images/salt/unet_128_v2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = evaluate([MODEL_PATH], train[train.fold.isin([0,1,2,3])].id.values, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = evaluate([MODEL_PATH,MODEL_PATH2], train.id.values, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{} / {} / {}\".format(np.round(np.mean(res['iout']),5),np.round(np.mean(res['dice']),5),np.round(np.mean(res['jacard']),5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.80788"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['iout'] = res['iout']\n",
    "train.groupby('fold').iout.aggregate('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = ensemble([MODEL_PATH,MODEL_PATH2],[0,1,2,3,4],test.id.values,0.5)\n",
    "# pred = ensemble([MODEL_PATH],[0,1,2,3,4],test.id.values,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['rle_mask'] = pred\n",
    "test.to_csv('ens_1_9_80788.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python unets.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot2x2Array(image, mask):\n",
    "    f, axarr = plt.subplots(1,2)\n",
    "    axarr[0].imshow(image)\n",
    "    axarr[1].imshow(mask)\n",
    "    axarr[0].grid()\n",
    "    axarr[1].grid()\n",
    "    axarr[0].set_title('Image')\n",
    "    axarr[1].set_title('Mask')\n",
    "    \n",
    "for i in range(5):\n",
    "    image, mask = dataset[np.random.randint(0, len(dataset))]\n",
    "    plot2x2Array(image, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(idx):\n",
    "    from rle import rle_decode\n",
    "    img = rle_decode(pred[idx],(101,101))\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(cv2.imread('train/images/{}.png'.format(ids_valid[idx])))\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(cv2.imread('train/masks/{}.png'.format(ids_valid[idx])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    show_results(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/sample_submission.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = '/home/branding_images/salt/initial_fold_0.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = get_unet_128(input_shape=(128, 128, 3),num_classes=1)\n",
    "#model = get_unet_128_kaggle(input_shape=(128, 128, 3),start_neurons=16,num_classes=1)\n",
    "model.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = predict_test(model=model,\n",
    "                    preds_path='/home/branding_images/salt/initial_fold_0',\n",
    "                    ids=test.id.values,\n",
    "                    batch_size=BATCH_SIZE*4,\n",
    "                    thr=0.5,\n",
    "                    TTA='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(5):\n",
    "    weights_path = '/home/branding_images/salt/initial_fold_{}.hdf5'.format(fold)\n",
    "    model.load_weights(weights_path)\n",
    "    \n",
    "    dir_path = '/home/branding_images/salt/initial_fold_{}'.format(fold)\n",
    "    os.system(\"mkdir {}\".format(dir_path))\n",
    "    pred = predict_test(model=model,\n",
    "                    preds_path=dir_path,\n",
    "                    ids=test.id.values,\n",
    "                    batch_size=BATCH_SIZE*4,\n",
    "                    thr=0.5,\n",
    "                    TTA='')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['rle_mask'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([0.8977854151784638,\n",
    " 0.8849150832019635,\n",
    " 0.8765836107451583,\n",
    " 0.9021209366713898,\n",
    " 0.8764196126324356])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial_generator (not actually true)\n",
    "dices\n",
    "[0.8977854151784638,\n",
    " 0.8849150832019635,\n",
    " 0.8765836107451583,\n",
    " 0.9021209366713898,\n",
    " 0.8764196126324356]\n",
    "\n",
    "VALIDATION: 0.762575\n",
    "+ TTA. HorizontalFlip: 0.77545\n",
    "TTA. VerticalFlip: 0.770274\n",
    "Vertical+Horizontal: 0.7754 \n",
    "LB: 0.776\n",
    "TTA. LB: 0.777 :(\n",
    "    \n",
    "fold 0: 0.785309   \n",
    "LB: 0.748\n",
    "\n",
    "fold\n",
    "0    0.785309\n",
    "1    0.750373\n",
    "2    0.777559\n",
    "3    0.767215\n",
    "4    0.731465\n",
    "\n",
    "0    0.791481\n",
    "1    0.789552\n",
    "2    0.805302\n",
    "3    0.795949    \n",
    "    \n",
    "epochs\n",
    "[40, 48, 23, 39, 46]\n",
    "\n",
    "losses\n",
    "[0.26385884437664053,\n",
    " 0.28554327216981656,\n",
    " 0.29580774510097857,\n",
    " 0.2410078847427157,\n",
    " 0.31404423417559096]\n",
    "    \n",
    "unet_128:\n",
    "dices\n",
    "\n",
    "[0.8928389563972567,\n",
    " 0.8714483881471169,\n",
    " 0.875522751475968,\n",
    " 0.8969466935230207,\n",
    " 0.8706270683343243]\n",
    "\n",
    "epochs\n",
    "\n",
    "[54, 33, 51, 38, 30]\n",
    "\n",
    "losses\n",
    "\n",
    "[0.27717545788597175,\n",
    " 0.30893601562401546,\n",
    " 0.33097895994580334,\n",
    " 0.24521447161707696,\n",
    " 0.2972317286357758]\n",
    "    \n",
    "CV: 0.757875\n",
    "LB: 0.771\n",
    "    \n",
    "fold_0: 0.762716\n",
    "LB: 0.727\n",
    "    \n",
    "fold\n",
    "0    0.762716\n",
    "1    0.743159\n",
    "2    0.770284\n",
    "3    0.758228\n",
    "4    0.754777\n",
    "    \n",
    "\n",
    "adam_do_fold_0:\n",
    "0.7662962962962964\n",
    "LB: 0.751"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(dices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([0.8928389563972567,\n",
    " 0.8714483881471169,\n",
    " 0.875522751475968,\n",
    " 0.8969466935230207,\n",
    " 0.8706270683343243])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(dices1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(dices2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('initial_genenerator_5_folds_887565.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rle import rle_decode\n",
    "img = rle_decode(pred[1],(101,101))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.imread('test/images/{}.png'.format(test.id.values[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('test/images/{}.png'.format(test.id.values[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNET d4 augs (0.62)\n",
    "LINKNET (0.70)\n",
    "cv.gaussian blur for masks predicted smooting\n",
    "absolutely black images -- drop in train. Null in test\n",
    "\n",
    "loss: iou is overprediction. bce+jaccard\n",
    "we could use 96 or 128 size.\n",
    "bce+jaccard: 0.15 on validation; iou -- 0.8. LB 0.74\n",
    "\n",
    "All white and all black images in train and test!\n",
    "\n",
    "Should I use noi3e's folds? Yes, just dropping constant images during training and imputing during validation\n",
    "\n",
    "Start with predicting: whether an image has mask at all -- binary classification.\n",
    "    Then multiply these prior probabilities on mask obtained! Again: classification pipeline.\n",
    "        \n",
    "Some 100% incorrect masks!\n",
    "\n",
    "FIND DUPLICATES OF IMAGES!\n",
    "\n",
    "Change Kaggle architectures with comments from ods:\n",
    "Use SpatialDropout2D and decrease dropout rate\n",
    "Conv2DTranspose works worse than NN upsampling + conv\n",
    "\"You don't need multiprocessing, it will run batches preparation in different processes\"\n",
    "-- use multiprocessing in keras\n",
    "\n",
    "\n",
    "\n",
    "TRY TO BINIRIZE MASK AFTER APPLYING RESIZE\n",
    "HOW IS MY DICE CALCULATED? IT SHOULD HAVE THRESHOLD\n",
    "\n",
    "DECREASE LR? Too noisy val. Change to rmsprop. DICE is pretty much stable. But loss is jumping\n",
    "Ask question in ODS if won't manage\n",
    "Also aks about augmentations for segmentaion. Shoud we use inverse transform for the masks predicted?\n",
    "\n",
    "Increase Smoothing Parameter in Dice!\n",
    "\n",
    "Write Own Cyclic LR? Custom with saving checkpoints!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Try to use own reduceLR. Because it's not clear where keras starts with decreasing LR\n",
    "(I guess, from the last epoch. Not the best! So, reinitialize the model. Add some functions to not repeat the code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras_iterator import ImageDataGenerator\n",
    "from losses import make_loss, dice_coef_clipped, dice_coef\n",
    "from models import get_unet_resnet\n",
    "from random_transform_mask import ImageWithMaskFunction\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "img_height = 1280\n",
    "img_width = 1918\n",
    "out_height = 1280\n",
    "out_width = 1918\n",
    "input_height = 1024\n",
    "input_width = 1024\n",
    "use_crop = True\n",
    "learning_rate = 0.00001\n",
    "batch_size = 1\n",
    "nbr_train_samples = 4576\n",
    "nbr_validation_samples = 512\n",
    "freeze_till_layer = \"input_1\"\n",
    "nbr_epochs = 30\n",
    "dataset_dir = '/home/selim/kaggle/datasets/carvana'\n",
    "mask_dir = os.path.join(dataset_dir, \"train_masks\")\n",
    "val_mask_dir = os.path.join(dataset_dir, \"train_masks\")\n",
    "models_dir = '/home/selim/kaggle/models/carvana/resnet_2'\n",
    "best_model_file = models_dir + \"/resnet-refine-\" + str(input_width) + format(learning_rate, \".6f\") + \"-{epoch:d}-{val_loss:0.7f}-{val_dice_coef_clipped:0.7f}.h5\"\n",
    "train_data_dir = os.path.join(dataset_dir, 'train_split_2')\n",
    "val_data_dir = os.path.join(dataset_dir, 'train_val_2')\n",
    "weights = \"weights/resnet-on-test-combined-19200.000010-0-0.0037752-99.6908383.h5\"\n",
    "loss_function = \"boot_hard\"\n",
    "def freeze_model(model, freeze_before_layer):\n",
    "    if freeze_before_layer == \"ALL\":\n",
    "        for l in model.layers:\n",
    "            l.trainable = False\n",
    "    else:\n",
    "        freeze_before_layer_index = -1\n",
    "        for i, l in enumerate(model.layers):\n",
    "            if l.name == freeze_before_layer:\n",
    "                freeze_before_layer_index = i\n",
    "        for l in model.layers[:freeze_before_layer_index]:\n",
    "            l.trainable = False\n",
    "def preprocess_input_resnet(x, data_format=None):\n",
    "    \"\"\"Preprocesses a tensor encoding a batch of images.\n",
    "    # Arguments\n",
    "        x: input Numpy tensor, 4D.\n",
    "        data_format: data format of the image tensor.\n",
    "    # Returns\n",
    "        Preprocessed tensor.\n",
    "    \"\"\"\n",
    "    if data_format is None:\n",
    "        data_format = K.image_data_format()\n",
    "    assert data_format in {'channels_last', 'channels_first'}\n",
    "    if data_format == 'channels_first':\n",
    "        # 'RGB'->'BGR'\n",
    "        x = x[:, ::-1, :, :]\n",
    "        # Zero-center by mean pixel\n",
    "        x[0, :, :] -= 103.939\n",
    "        x[1, :, :] -= 116.779\n",
    "        x[2, :, :] -= 123.68\n",
    "    else:\n",
    "        # 'RGB'->'BGR'\n",
    "        x = x[:, :, ::-1]\n",
    "        # Zero-center by mean pixel\n",
    "        x[:, :, 0] -= 103.939\n",
    "        x[:, :, 1] -= 116.779\n",
    "        x[:, :, 2] -= 123.68\n",
    "    return x\n",
    "def preprocess_input(x):\n",
    "    return preprocess_input_resnet(x)\n",
    "model = get_unet_resnet((input_height, input_width, 3))\n",
    "freeze_model(model, freeze_till_layer)\n",
    "if weights is not None:\n",
    "    model.load_weights(weights)\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "model.summary()\n",
    "model.compile(loss=make_loss(loss_function), optimizer=optimizer, metrics=[dice_coef, binary_crossentropy, dice_coef_clipped])\n",
    "model.summary()\n",
    "crop_size = None\n",
    "if use_crop:\n",
    "    crop_size = (input_height, input_width)\n",
    "mask_function = ImageWithMaskFunction(out_size=(out_height, out_width), crop_size=crop_size, mask_dir=val_mask_dir)\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    classes=None,\n",
    "    class_mode='regression',\n",
    "    output_function=ImageWithMaskFunction(out_size=(out_height, out_width), crop_size=crop_size, mask_dir=mask_dir).mask_pred_train)\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    classes=None,\n",
    "    class_mode='regression', output_function=mask_function.mask_pred_val)\n",
    "best_model = ModelCheckpoint(best_model_file, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=True)\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nbr_train_samples / batch_size + 1,\n",
    "    epochs=nbr_epochs,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=nbr_validation_samples / batch_size + 1,\n",
    "    callbacks=[best_model, EarlyStopping(patience=45, verbose=10)], workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
