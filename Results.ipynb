{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cyclic LR\n",
    "# T = nb_epochs\n",
    "# M = nb_snapshots\n",
    "# alpha_zero = init_lr\n",
    "\n",
    "T = 200\n",
    "M = 4\n",
    "alpha_zero = 0.0001\n",
    "\n",
    "import numpy as np\n",
    "t = 45\n",
    "\n",
    "cos_inner = np.pi * (t % (T // M))  # t - 1 is used when t has 1-based indexing.\n",
    "cos_inner /= T // M\n",
    "cos_out = np.cos(cos_inner) + 1\n",
    "max(float(alpha_zero / 2 * cos_out),0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_resnet_152_192_224_snapshot_100_epochs_bs_16_finetune_lovash_v1 (LB: 0.847)\n",
    "                    iout      dice    jacard\n",
    "coverage_class                              \n",
    "0               0.982372  0.977680  0.977680\n",
    "1               0.320000  0.491966  0.441499\n",
    "2               0.475862  0.567506  0.494737\n",
    "3               0.695402  0.841715  0.781787\n",
    "4               0.848837  0.936535  0.893549\n",
    "5               0.920755  0.960154  0.933644\n",
    "6               0.968235  0.974113  0.961609\n",
    "7               0.987755  0.930919  0.923772\n",
    "Without 0 class:  0.81024 / 0.86418 / 0.83151\n",
    "0.87654 / 0.9079 / 0.88781"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./predict_all.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./evaluate_all.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2\n",
      "1 2\n",
      "2 2\n",
      "3 2\n"
     ]
    }
   ],
   "source": [
    "for a, b in enumerate((2, 2, 2, 2)):\n",
    "    print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CSSE+ResNext\n",
    "unet_resnext_50_192_224_reduce_lr_smaller_decoder_bce_dice_csse_usual -- 5 hours\n",
    "Without 0 class:  0.80843 / 0.87256 / 0.83889\n",
    "0.87235 / 0.90693 / 0.88622 (LB: 0.841)\n",
    "    unet_resnext_50_lovasz_192_224_reduce_lr_smaller_decoder_bce_dice_csse_usual\n",
    "    Without 0 class:  0.80482 / 0.87167 / 0.83611\n",
    "    0.86988 / 0.90638 / 0.88451\n",
    "    \n",
    "    unet_resnext_50_lovasz_192_224_reduce_lr_smaller_decoder_bce_dice_csse_usual_00005\n",
    "    Without 0 class:  0.81426 / 0.87224 / 0.83945 -- 7 hours\n",
    "    0.87716 / 0.90807 / 0.88791 (LB: 0.851)\n",
    "            \n",
    "        unet_resnext_50_lovasz_192_224_reduce_lr_smaller_decoder_bce_dice_csse_usual_00005_finetune_snapshots_low_thr_v1\n",
    "        Without 0 class:  0.82149 / 0.88126 / 0.84793\n",
    "        0.88333 / 0.91477 / 0.89429 (LB: 0.850 ?!)\n",
    "        Try 5 folds: \n",
    "        \n",
    "        unet_resnext_50_lovasz_192_224_reduce_lr_smaller_decoder_bce_dice_csse_usual_00005_finetune_snapshots_low_thr_v2\n",
    "        Without 0 class:  0.81847 / 0.87481 / 0.84199\n",
    "        0.87827 / 0.90902 / 0.88884\n",
    "        \n",
    "        unet_resnext_50_lovasz_192_224_reduce_lr_smaller_decoder_bce_dice_csse_usual_00005_finetune_snapshots_low_thr_v3\n",
    "        Without 0 class:  0.81566 / 0.87539 / 0.842\n",
    "        0.87457 / 0.90632 / 0.88579 (LB: 0.852)\n",
    "        \n",
    "        3 snapshots:\n",
    "        0.88 / 0.91122 / 0.89096\n",
    "        \n",
    "        3 snapshots + initial:\n",
    "        0.88148 / 0.91057 / 0.89055 (LB: 0.854)\n",
    "        thr 0.45: 0.88457 / 0.9118 / 0.89183 (LB: 0.854)\n",
    "            \n",
    "        1st snapshot + initial:\n",
    "        0.88247 / 0.91229 / 0.89239 (LB: 0.852)\n",
    "        \n",
    "        \n",
    "        unet_resnext_50_lovasz_192_224_reduce_lr_smaller_decoder_bce_dice_csse_usual_00005_finetune_snapshots_00001\n",
    "        Without 0 class:  0.81265 / 0.87334 / 0.84016\n",
    "        0.87617 / 0.90942 / 0.88902\n",
    "        \n",
    "        unet_resnext_50_lovasz_192_224_reduce_lr_smaller_decoder_bce_dice_csse_usual_00005_finetune_snapshots_low_thr_0001_v1\n",
    "        Without 0 class:  0.81145 / 0.87279 / 0.83956\n",
    "        0.87728 / 0.91137 / 0.89094\n",
    "        \n",
    "        unet_resnext_50_lovasz_192_224_reduce_lr_smaller_decoder_bce_dice_csse_usual_00005_finetune_snapshots_low_thr_0001_v2\n",
    "        Without 0 class:  0.81707 / 0.875 / 0.84188\n",
    "        0.8821 / 0.91207 / 0.8917\n",
    "        \n",
    "        unet_resnext_50_lovasz_192_224_reduce_lr_smaller_decoder_bce_dice_csse_usual_00005_finetune_snapshots_low_thr_0001_v3\n",
    "        Without 0 class:  0.81586 / 0.87301 / 0.84036\n",
    "        0.88123 / 0.91095 / 0.89088\n",
    "        \n",
    "        v1+3:\n",
    "        0.88494 / 0.9158 / 0.89554 (LB: 0.854)\n",
    "        \n",
    "        initial+v1+3:\n",
    "        0.88333 / 0.91419 / 0.89423 (LB: 0.855)\n",
    "            \n",
    "            asd\n",
    "\n",
    "\n",
    "    \n",
    "    unet_resnext_50_lovasz_192_224_reduce_lr_smaller_decoder_bce_dice_csse_usual_snapshots\n",
    "    Without 0 class:  0.81566 / 0.87316 / 0.8395\n",
    "    0.87679 / 0.90993 / 0.88924\n",
    "    \n",
    "        unet_resnext_50_lovasz_192_224_reduce_lr_smaller_decoder_bce_dice_csse_usual_2nd_finetune_snapshots -- 4 hours\n",
    "        Without 0 class:  0.81526 / 0.87374 / 0.84068\n",
    "        0.87901 / 0.91011 / 0.88978 (LB: 0.850)\n",
    "        \n",
    "\n",
    "# Corrected SCSE\n",
    "unet_resnext_50_192_224_reduce_lr_smaller_decoder_bce_dice_csse_corrected\n",
    "Without 0 class:  0.80622 / 0.86356 / 0.82998\n",
    "0.87074 / 0.90287 / 0.88222\n",
    "        \n",
    "# Snapshots in first phase\n",
    "unet_resnext_50_192_224_reduce_lr_smaller_decoder_bce_dice_csse_snapshot\n",
    "Without 0 class:  0.79679 / 0.86858 / 0.83261\n",
    "0.85951 / 0.8962 / 0.87409        \n",
    "    \n",
    "# x2 decoder capacity\n",
    "# BATCH SIZE 16\n",
    "- unet_resnext_50_192_224_reduce_lr_bce_dice_csse_usual \n",
    "Without 0 class:  0.78956 / 0.85579 / 0.82092\n",
    "0.85432 / 0.89163 / 0.87019\n",
    "\n",
    "# /2 decoder capacity. Good performance on non-zero class!\n",
    "# BATCH SIZE 16\n",
    "unet_resnext_50_192_224_reduce_lr_bce_dice_csse_usual_x4_dec_capacity\n",
    "Without 0 class:  0.81024 / 0.87665 / 0.84175\n",
    "0.86741 / 0.90344 / 0.88199\n",
    "# Try with batch size 24\n",
    "\n",
    "# Add hypercolumn to this very small decoder\n",
    "\n",
    "# 128 padding:\n",
    "unet_resnext_50_101_128_reduce_lr_smaller_decoder_bce_dice_csse_usual\n",
    "Without 0 class:  0.79056 / 0.85481 / 0.81954\n",
    "0.85642 / 0.89376 / 0.87208\n",
    "\n",
    "\n",
    "# What is the first: this one or faster convergence?\n",
    "# Look at Heng's architecture in the slides! All are training with 128, try also! Now with ResNext and deleting some layers\n",
    "# Start with just 128 training as usual (padding)\n",
    "# Then dances with buben\n",
    "\n",
    "# START OVER WITH NEW FOLDS DISTRIBUTION!\n",
    "unet_resnext_50_192_224_mosaic_folds_initial -- 4 hours\n",
    "Without 0 class:  0.80427 / 0.88112 / 0.84599\n",
    "0.87214 / 0.91456 / 0.89315 (LB: 0.839)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_resnet_34_192_224_reduce_lr_smaller_decoder_bce_dice_csse_usual\n",
    "Without 0 class:  0.79297 / 0.86288 / 0.82612\n",
    "0.85519 / 0.89721 / 0.8746\n",
    "\n",
    "unet_resnet_50_192_224_reduce_lr_smaller_decoder_bce_dice_csse_usual\n",
    "Without 0 class:  0.79859 / 0.85655 / 0.82264\n",
    "0.86247 / 0.89536 / 0.87451\n",
    "\n",
    "unet_resnet_152_192_224_reduce_lr_smaller_decoder_bce_dice_csse_usual -- 6 hours\n",
    "Without 0 class:  0.80462 / 0.86957 / 0.83549\n",
    "0.86963 / 0.90667 / 0.88571\n",
    "\n",
    "unet_resnext_50_192_224_reduce_lr_smaller_decoder_bce_dice_csse_usual -- 5 hours\n",
    "Without 0 class:  0.80843 / 0.87256 / 0.83889\n",
    "0.87235 / 0.90693 / 0.88622 (LB: 0.841)\n",
    "\n",
    "?! relly? Deleting CSSE?\n",
    "unet_resnext_50_192_224_reduce_lr_smaller_decoder_bce_dice\n",
    "Without 0 class:  0.7998 / 0.8661 / 0.83104\n",
    "0.85951 / 0.89598 / 0.87442\n",
    "\n",
    "unet_resnext_101_192_224_reduce_lr_smaller_decoder_bce_dice_csse_usual\n",
    "Without 0 class:  0.81185 / 0.87769 / 0.84322\n",
    "0.8742 / 0.90885 / 0.88766\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_resnet_34_lovasz_192_224_reduce_lr_smaller_decoder_bce_dice_csse_150_epochs_2_snapshots\n",
    "                    iout      dice    jacard\n",
    "coverage_class                              \n",
    "0               0.979167  0.971537  0.971537\n",
    "1               0.326000  0.526553  0.467718\n",
    "2               0.513793  0.493366  0.440939\n",
    "3               0.671264  0.824259  0.763522\n",
    "4               0.783721  0.892881  0.843500\n",
    "5               0.908491  0.951208  0.922942\n",
    "6               0.975294  0.975143  0.963399\n",
    "7               0.987755  0.930276  0.922709\n",
    "Without 0 class:  0.80181 / 0.85466 / 0.82132\n",
    "0.87012 / 0.89968 / 0.87918\n",
    "\n",
    "unet_resnet_34_192_224_reduce_lr_smaller_decoder_bce_dice_csse_150_epochs\n",
    "                    iout      dice    jacard\n",
    "coverage_class                              \n",
    "0               0.959615  0.948137  0.948137\n",
    "1               0.276000  0.485039  0.423885\n",
    "2               0.493103  0.529791  0.471785\n",
    "3               0.655172  0.827251  0.762470\n",
    "4               0.790698  0.906530  0.856387\n",
    "5               0.905660  0.954528  0.923772\n",
    "6               0.968235  0.972683  0.959102\n",
    "7               0.972449  0.929431  0.919964\n",
    "Without 0 class:  0.78855 / 0.85444 / 0.81855\n",
    "0.85444 / 0.89053 / 0.86846\n",
    "\n",
    "+ unet_resnet_34_192_224_reduce_lr_smaller_decoder_bce_dice\n",
    "0.85728 / 0.9015 / 0.87891 (LB: 0.830)\n",
    "                    iout      dice    jacard\n",
    "coverage_class                              \n",
    "0               0.961538  0.956034  0.956034\n",
    "1               0.296000  0.502172  0.442301\n",
    "2               0.579310  0.680925  0.616110\n",
    "3               0.662069  0.833376  0.768325\n",
    "4               0.795349  0.918964  0.866920\n",
    "5               0.899057  0.955525  0.922967\n",
    "6               0.971765  0.973734  0.960369\n",
    "7               0.950000  0.928621  0.919028\n",
    "Without 0 class:  0.79197 / 0.86733 / 0.83059\n",
    "\n",
    "unet_resnet_34_192_224_reduce_lr_smaller_decoder_bce_dice_again\n",
    "0.85481 / 0.88778 / 0.86649\n",
    "                   iout      dice    jacard\n",
    "coverage_class                              \n",
    "0               0.957692  0.939444  0.939444\n",
    "1               0.274000  0.467213  0.411769\n",
    "2               0.600000  0.674885  0.608037\n",
    "3               0.637931  0.798052  0.737160\n",
    "4               0.781395  0.892756  0.845364\n",
    "5               0.900000  0.946533  0.916776\n",
    "6               0.970588  0.973056  0.959597\n",
    "7               0.974490  0.940801  0.931661\n",
    "Without 0 class:  0.79036 / 0.85541 / 0.82079\n",
    "    \n",
    "\n",
    "+ unet_resnet_34_192_224_reduce_lr_smaller_decoder_bce_dice_hypecolumn_corr\n",
    "                    iout      dice    jacard\n",
    "coverage_class                              \n",
    "0               0.961218  0.953603  0.953603\n",
    "1               0.306000  0.505671  0.448908\n",
    "2               0.551724  0.671891  0.593116\n",
    "3               0.649425  0.823718  0.758096\n",
    "4               0.804651  0.912959  0.865027\n",
    "5               0.903774  0.952293  0.922493\n",
    "6               0.967059  0.972940  0.959259\n",
    "7               0.981633  0.932344  0.925106\n",
    "Without 0 class:  0.79639 / 0.86486 / 0.82887\n",
    "0.85988 / 0.89904 / 0.87692\n",
    "\n",
    "unet_resnet_34_192_224_reduce_lr_smaller_decoder_bce_dice_hypecolumn_csse\n",
    "                    iout      dice    jacard\n",
    "coverage_class                              \n",
    "0               0.958333  0.949964  0.949964\n",
    "1               0.314000  0.505392  0.451181\n",
    "2               0.541379  0.703059  0.624437\n",
    "3               0.663218  0.824442  0.761406\n",
    "4               0.823256  0.918310  0.872014\n",
    "5               0.908491  0.955539  0.925832\n",
    "6               0.969412  0.973220  0.959898\n",
    "7               0.971429  0.937598  0.928082\n",
    "Without 0 class:  0.8 / 0.86901 / 0.83351\n",
    "0.86099 / 0.90019 / 0.87837 (LB: 0.832)\n",
    "\n",
    "unet_resnet_34_lovasz_192_224_reduce_lr_smaller_decoder_bce_dice_hypecolumn_csse\n",
    "                    iout      dice    jacard\n",
    "coverage_class                              \n",
    "0               0.983013  0.974818  0.974818\n",
    "1               0.316000  0.517470  0.457275\n",
    "2               0.551724  0.581584  0.524814\n",
    "3               0.664368  0.833628  0.770387\n",
    "4               0.832558  0.933500  0.886830\n",
    "5               0.907547  0.951777  0.922162\n",
    "6               0.969412  0.973718  0.960732\n",
    "7               0.970408  0.938069  0.927974\n",
    "Without 0 class:  0.80141 / 0.86544 / 0.83051\n",
    "0.87136 / 0.90757 / 0.88609 (LB: 0.844)\n",
    "\n",
    "\n",
    "+ unet_resnet_34_lovasz_192_224_reduce_lr_smaller_decoder_bce_dice_lovash\n",
    "                    iout      dice    jacard\n",
    "coverage_class                              \n",
    "0               0.962179  0.946042  0.946042\n",
    "1               0.318000  0.514463  0.458329\n",
    "2               0.537931  0.712627  0.633097\n",
    "3               0.670115  0.841245  0.775721\n",
    "4               0.825581  0.934899  0.885229\n",
    "5               0.908491  0.952803  0.923772\n",
    "6               0.971765  0.974392  0.961773\n",
    "7               0.983673  0.938800  0.930608\n",
    "Without 0 class:  0.80442 / 0.8747 / 0.83875\n",
    "0.86519 / 0.90218 / 0.88008 (LB: 0.842)\n",
    "    \n",
    "unet_resnet_34_lovasz_192_224_reduce_lr_smaller_decoder_bce_dice_lovash_elu_1\n",
    "                    iout      dice    jacard\n",
    "coverage_class                              \n",
    "0               0.974038  0.961632  0.961632\n",
    "1               0.298000  0.491124  0.434524\n",
    "2               0.562069  0.607285  0.549526\n",
    "3               0.666667  0.832155  0.768619\n",
    "4               0.820930  0.937483  0.889291\n",
    "5               0.898113  0.950487  0.919954\n",
    "6               0.968235  0.973330  0.960050\n",
    "7               0.993878  0.931985  0.925706\n",
    "Without 0 class:  0.80201 / 0.86284 / 0.82853\n",
    "0.86827 / 0.9009 / 0.8798 (LB: 0.843)\n",
    "\n",
    "unet_resnet_34_lovasz_192_224_reduce_lr_smaller_decoder_bce_dice_lovash_relu_per_image_false\n",
    "                    iout      dice    jacard\n",
    "coverage_class                              \n",
    "0               0.961538  0.955728  0.955728\n",
    "1               0.328000  0.516691  0.461662\n",
    "2               0.510345  0.562830  0.509533\n",
    "3               0.663218  0.828700  0.764307\n",
    "4               0.797674  0.909294  0.860143\n",
    "5               0.894340  0.950896  0.919188\n",
    "6               0.964706  0.971822  0.957159\n",
    "7               0.971429  0.932665  0.924967\n",
    "Without 0 class:  0.79357 / 0.85975 / 0.82486\n",
    "0.85827 / 0.89672 / 0.87527\n",
    "\n",
    "unet_resnet_34_lovasz_192_224_reduce_lr_smaller_decoder_bce_dice_lovash_snapshot_finetune\n",
    "                    iout      dice    jacard\n",
    "coverage_class                              \n",
    "0               0.969872  0.952189  0.952189\n",
    "1               0.312000  0.525845  0.465413\n",
    "2               0.510345  0.637400  0.563377\n",
    "3               0.682759  0.838070  0.777039\n",
    "4               0.813953  0.923886  0.874723\n",
    "5               0.902830  0.954107  0.924506\n",
    "6               0.968235  0.974145  0.961561\n",
    "7               0.973469  0.934464  0.926840\n",
    "Without 0 class:  0.7996 / 0.86934 / 0.83411 \n",
    "0.86519 / 0.90125 / 0.87959 (LB: 0.841)\n",
    "    \n",
    "unet_resnet_34_101_128_reduce_lr_smaller_decoder_bce_dice_corr\n",
    "                    iout      dice    jacard\n",
    "coverage_class                              \n",
    "0               0.955128  0.946512  0.946512\n",
    "1               0.244000  0.444968  0.392067\n",
    "2               0.544828  0.645958  0.575064\n",
    "3               0.611494  0.785518  0.720898\n",
    "4               0.772093  0.903461  0.848491\n",
    "5               0.885849  0.952564  0.917940\n",
    "6               0.965882  0.970862  0.955342\n",
    "7               0.986735  0.929521  0.921061\n",
    "Without 0 class:  0.77731 / 0.84891 / 0.81175\n",
    "0.8458 / 0.88651 / 0.86366    \n",
    " \n",
    "unet_resnet_34_101_128_reduce_lr_smaller_decoder_bce_dice_corr\n",
    "                    iout      dice    jacard\n",
    "coverage_class                              \n",
    "0               0.955128  0.946512  0.946512\n",
    "1               0.244000  0.444968  0.392067\n",
    "2               0.544828  0.645958  0.575064\n",
    "3               0.611494  0.785518  0.720898\n",
    "4               0.772093  0.903461  0.848491\n",
    "5               0.885849  0.952564  0.917940\n",
    "6               0.965882  0.970862  0.955342\n",
    "7               0.986735  0.929521  0.921061\n",
    "Without 0 class:  0.77731 / 0.84891 / 0.81175\n",
    "0.8458 / 0.88651 / 0.86366        \n",
    "    \n",
    "# NO REDUCE IN LR!! :(\n",
    "unet_resnet_34_192_224_reduce_lr_smaller_decoder_bce_dice_hypecolumn\n",
    "                    iout      dice    jacard\n",
    "coverage_class                              \n",
    "0               0.968269  0.964943  0.964943\n",
    "1               0.218000  0.377154  0.334806\n",
    "2               0.420690  0.567644  0.494604\n",
    "3               0.613793  0.797461  0.731153\n",
    "4               0.806977  0.924168  0.872266\n",
    "5               0.900000  0.952110  0.920200\n",
    "6               0.975294  0.974189  0.961253\n",
    "7               0.936735  0.931763  0.919094\n",
    "Without 0 class:  0.76566 / 0.84233 / 0.80627\n",
    "0.8437 / 0.88956 / 0.86739    \n",
    "\n",
    "Use correct version of CSSE! For all the following models\n",
    "\n",
    "Increase cycle sizes while running shapshots ensembling\n",
    "\n",
    "We need faster convergence to the same result\n",
    "\n",
    "Make all the experiments on the first phase only!\n",
    "Change the experimenting fold?! Yes!\n",
    "\n",
    "Hard Examples Mining during finetuning (from 0-5 groups!)\n",
    "\n",
    "Finetuning on larger size\n",
    "\n",
    "\n",
    "Try to decrease image size to 128 (with padding)\n",
    "Try do delete first max pooling\n",
    "\n",
    "Drop verticals from training. And fill them in only with leakage?\n",
    "\n",
    "Add initial image to the decoder\n",
    "\n",
    "Drop skip connections in lower resolution, while increase depth in higher resolutions\n",
    "\n",
    "\n",
    "Try 2nd stage finetunig wit BCE?\n",
    "\n",
    "Try usual Unet-ResNet50.\n",
    "\n",
    "ResNet34 train more. 150+100+... It was incorrect :(\n",
    "\n",
    "ResNext50 without CSSE\n",
    "\n",
    "ResNext101 for stacking!    \n",
    "    \n",
    "    \n",
    "Drop HyperColumn. Keep CSSE. Run with bce_dice for 150 epochs with constant LR -- 3 hours\n",
    "Then, lovasz with 100 epochs with constant LR/2 -- 7 hours, 20 mins\n",
    "Then, maybe, finetune with snapshots with LR/10 -- 2x20? -- 3 hours\n",
    "\n",
    "Tweak some parameters in UNET decoder (like convs 5x5), add dropout, etc.\n",
    "\n",
    "Padding with another border type!\n",
    "Other augmentations from Ternaus! Check by hand\n",
    "    \n",
    "Look at Predictions and Possible Mosaics!\n",
    "Look at augmentations with Ternaus Notebook\n",
    "\n",
    "- Try depth channels. It seems random\n",
    "The depth may correspond with a relative depth with the ground, which is itself not flat ?\n",
    "\n",
    "Select best Epoch based on non-zero LB metric! Zeroes could be done with classification in the worst case (or deleting small masks)\n",
    "\n",
    "Focal loss\n",
    "\n",
    "Try to freeze encoder again for a couple of epochs\n",
    "\n",
    "Add initial image before last conv\n",
    "\n",
    "SE blocks (in the end; in decoder steps!; in encoder and decoder steps?)\n",
    "If adding in Encoder, load_weights by name\n",
    "\n",
    "Decrease Batch_Size to 16\n",
    "\n",
    "Try other small backbones like ResNeXt50 and DenseNet121\n",
    "\n",
    "Backbones from the Keras new Release\n",
    "\n",
    "Trainig More with Fixed LR (without dropping it. It could accidenely go higher)\n",
    "\n",
    "did you check train/test loss?\n",
    "- large size salt loss is large: change encoder and decoder or decrese parameter\n",
    "- small size salt loss is large: use dilation\n",
    "\n",
    "Extend middle block with dilation or just more conv layers\n",
    "\n",
    "WHY to resize during inference?! Selim's Answer in Deep Learning\n",
    "Try to delete resize\n",
    "\n",
    "\n",
    "# Somewhat True\n",
    "202 -> 256\n",
    "Check this. Compare quality on different mask sizes.\n",
    "!Only small mask ( cover of less than 50%) benefits from 256x256. You can use it as a refinement step for some selected ouput\n",
    "So, you could train 128x128. And then finetune with 256x256 only on images, where we're predicting small masks in the first phase!\n",
    "\n",
    "\n",
    "+! Fix Lovasz (elu+1). On the same pretrained weights as the first version!\n",
    "- try best Lovasz with per_image=False\n",
    "\n",
    "- Further FineTune with lovasz and snapshots (cosine annealing (save best epochs) -- top and bottom LR thresholds)\n",
    "3rd finetuning with more thresholds (from 0.0001 to 0.00001) like Pete. Check what you've been doing (0.00005?)\n",
    "max(float(alpha_zero / 2 * cos_out),0.00001)\n",
    "\n",
    "\n",
    "\n",
    "- FPN\n",
    "- Weighted Dice-BCE loss (more weight to dice)\n",
    "- Augmentations:\n",
    "    - Add rotation\n",
    "    - Add normalization\n",
    "- Try Adam (0.0001) and SGD (0.01) optimizers \n",
    "\n",
    "    \n",
    "Run 2 folds?\n",
    "\n",
    "\n",
    "Try ResNet152 backbone out-of-box\n",
    "Increase Decoder's size\n",
    "Revert to old ResNet152?..\n",
    "Finetune old ResNet152 with higher size?\n",
    "\n",
    "Train multiple models simultaneously with batch size // 2?\n",
    "\n",
    "\n",
    "Random crops from concatenated images anyhow\n",
    "\n",
    "Attention\n",
    "\n",
    "PSP\n",
    "\n",
    "LINKNET\n",
    "\n",
    "HYPERCOLUMN\n",
    "\n",
    "Pretrain Encoder on Classification Problem (only horizontal flipping, or all D4? With transpositions)\n",
    "Or multiply on probabilities obtained\n",
    "\n",
    "Reproduce Pete's first phase\n",
    "\n",
    "Add pseudolabels from the best old model (0.857).\n",
    "Probably, with mosaic crops\n",
    "\n",
    "- Train on smaller size. Finetune on larger!!\n",
    "\n",
    "Majority Voting for Empty Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    'unet_resnet_34_lovasz_192_224_reduce_lr_smaller_decoder_bce_dice_lovash_elu_1',\n",
    "    'unet_resnet_34_lovasz_192_224_reduce_lr_smaller_decoder_bce_dice_lovash'\n",
    "         ]\n",
    "fold 0: 0.86864 / 0.90345 / 0.88189\n",
    "    \n",
    "    \n",
    "unet_resnet_34_lovasz_192_224_reduce_lr_smaller_decoder_bce_dice_hypecolumn_csse+\n",
    "unet_resnet_34_lovasz_192_224_reduce_lr_smaller_decoder_bce_dice_lovash_elu_1\n",
    "fold 0: 0.87383 / 0.9121 / 0.89072 (LB: 0.846)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_resnet_34_192_224_reduce_lr\n",
    "0.83852 / 0.87847 / 0.85588\n",
    "\n",
    "unet_resnet_34_192_224_reduce_lr_v2\n",
    "0.84716 / 0.88863 / 0.86725 (~LB: 0.813)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_resnet_50_lovasz_192_224_old_folds_finetune_lovash\n",
    "fold 0: 0.86605 / 0.90163 / 0.87934 (LB: 0.833)\n",
    "fold 1: 0.84527 / 0.89041 / 0.86752\n",
    "fold 2: 0.86769 / 0.90191 / 0.88076\n",
    "fold 3: 0.84291 / 0.89786 / 0.87398\n",
    "fold 4: 0.83656 / 0.86855 / 0.8477 (LB: 0.839)\n",
    "all folds: 0.85185 / 0.8922 / 0.86999 (LB: 0.847)\n",
    "\n",
    "unet_resnet_50_192_224_initial\n",
    "fold 0: 0.83788 / 0.88275 / 0.85973\n",
    "fold 1: 0.84439 / 0.87474 / 0.85272\n",
    "fold 2: 0.8412 / 0.88455 / 0.8611\n",
    "fold 3: 0.84298 / 0.88211 / 0.86082\n",
    "fold 4: 0.83788 / 0.87166 / 0.84954\n",
    "all folds: 0.84087 / 0.87916 / 0.85678\n",
    "    \n",
    "unet_resnet_50_lovasz_192_224_initial_finetune_lovash -- Loss is not Training :()\n",
    "fold 0: 0.84311 / 0.8838 / 0.86161 (LB: 0.839)\n",
    "fold 1: 0.85077 / 0.88368 / 0.86197 (LB: 0.840)\n",
    "fold 2: 0.85179 / 0.88946 / 0.86711 (LB: 0.838)\n",
    "fold 3: 0.84681 / 0.8798 / 0.8601 (LB: 0.837)\n",
    "fold 4: 0.84681 / 0.88174 / 0.86026 (LB: 0.835 -- the highest size of the file!)\n",
    "all folds: 0.84786 / 0.8837 / 0.86221 (LB: 0.847)\n",
    "    \n",
    "unet_resnet_50_no_stride_101_112:\n",
    "fold 1: 0.83788 / 0.87251 / 0.85044\n",
    "\n",
    "unet_resnet_50_no_stride_101_112_2_more_convblocks:\n",
    "fold 1: 0.8324 / 0.86833 / 0.84653\n",
    "\n",
    "unet_resnet_50_no_stride_112_112\n",
    "    resize to 112 best\n",
    "0.82385 / 0.86392 / 0.84041\n",
    "\n",
    "\n",
    "unet_resnet_50_fold_1_no_stride_192_224\n",
    "0.81633 / 0.85828 / 0.83503\n",
    "\n",
    "unet_resnet_50_fold_1_no_stride_128_128\n",
    "0.81173 / 0.85038 / 0.82679\n",
    "\n",
    "unet_resnet_50_fold_1_no_stride_101_128\n",
    "0.83584 / 0.87683 / 0.85386\n",
    "\n",
    "unet_resnet_50_lovasz_192_224_epochs_20_50_finetune_lovash\n",
    "fold 1: 0.86505 / 0.88687 / 0.86572 (LB: 0.839)\n",
    "\n",
    "\n",
    "drop another pooling?\n",
    "\n",
    "unet_resnet_50_192_224_initial_old_folds\n",
    "fold 0: 0.85914 / 0.89594 / 0.87464 (~LB: 0.830)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./predict_all.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7*117/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "50+20 experiments\n",
    "\n",
    "Initial:\n",
    "    \n",
    "    \n",
    "Delete Max Pooling and use 128 size (and previous zero-padding!)\n",
    "http://neural.profitero.local:8888/edit/babakhin/Branding/salt_old/kaggle-salt/models/segmentation_models/segmentation_models/backbones/classification_models/classification_models/resnet/builder.py\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet50 HUGE training:\n",
    "\n",
    "unet_resnet_50_exp_0_192_224_3_snapshots_auto_builder_v1:\n",
    "total best: 0.86827 / 0.90156 / 0.88085\n",
    "best snapshot_1 (fold_0.hdf5.048-0.14318-0.85322.hdf5): 0.85765 / 0.89989 / 0.87788\n",
    "best snapshot_2 (fold_0.hdf5.095-0.14556-0.85410.hdf5): 0.85938 / 0.90199 / 0.88018\n",
    "best snapshot_3 (fold_0.hdf5.150-0.16642-0.85864.hdf5): 0.8663 / 0.90052 / 0.88002\n",
    "    (fold_0.hdf5.146-0.16717-0.85864.hdf5): 0.86827 / 0.90156 / 0.88085\n",
    "AVG of 3 best snapshots: 0.86568 / 0.90353 / 0.88284\n",
    "    \n",
    "unet_resnet_50_exp_0_192_224_3_snapshots_auto_builder_v1_finetune_lovash:\n",
    "total best: 0.8679 / 0.90123 / 0.88126\n",
    "best snapshot_1 (fold_0.hdf5.047-0.46291-0.86734.hdf5): 0.86716 / 0.90438 / 0.88342\n",
    "best snapshot_2 (fold_0.hdf5.091-0.47312-0.86330.hdf5): 0.86753 / 0.90064 / 0.87982\n",
    "best snapshot_3 (fold_0.hdf5.129-0.35461-0.87137.hdf5): 0.8679 / 0.90123 / 0.88126\n",
    "AVG of 3 best snapshots: 0.87296 / 0.90602 / 0.88581\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_resnet_152_192_224_snapshot_100_epochs_bs_16_finetune_lovash_v1\n",
    "fold 0: 0.87654 / 0.9079 / 0.88781 (0.87086 / 0.90374 / 0.88364)\n",
    "fold 1: 0.86244 / 0.89544 / 0.87531, best 0.86107 / 0.89774 / 0.87722 (0.85261 / 0.89117 / 0.87089)\n",
    "fold 2: 0.88076 / 0.9076 / 0.88833 (0.87768 / 0.90352 / 0.88363)\n",
    "fold 3: 0.85519 / 0.90768 / 0.88504, best 0.85544 / 0.90634 / 0.88396 (0.84823 / 0.90183 / 0.87844)\n",
    "fold 4: 0.85338 / 0.88414 / 0.86438 (0.84561 / 0.87515 / 0.85511)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECK RESULTS FOR DIFFERENT MASK SIZES!!!\n",
    "TO COMPARE WHERE YOU ARE PROGRESSING\n",
    "INCLUDING EMPTY MASKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- unet_resnet_50_do1_exp_0_192_224_snapshot_50_epochs -- no DO\n",
    "best: 0.85148 / 0.89215 / 0.87051\n",
    "snapshot: 0.8621 / 0.89688 / 0.87551\n",
    "\n",
    "- unet_resnet_50_do2_exp_0_192_224_snapshot_50_epochs -- DO 0.4\n",
    "best: 0.85642 / 0.89644 / 0.87446\n",
    "snapshot: 0.85432 / 0.89493 / 0.87249\n",
    "    \n",
    "+ unet_resnet_50_do1_exp_0_192_224_snapshot_50_epochs_inc_capacity\n",
    "best: 0.85852 / 0.89498 / 0.8729\n",
    "snapshot: 0.86111 / 0.8961 / 0.87418\n",
    "    \n",
    "+ unet_resnet_50_do2_exp_0_192_224_snapshot_50_epochs_dec_capacity\n",
    "best: 0.85951 / 0.89818 / 0.8763\n",
    "snapshot: 0.86173 / 0.89536 / 0.87428\n",
    "\n",
    "    \n",
    "+ unet_resnet_50_do1_exp_0_192_224_snapshot_50_epochs_qubvel_auto_builder_v1\n",
    "best: 0.86185 / 0.89778 / 0.87643\n",
    "snapshot: 0.86432 / 0.89986 / 0.87872\n",
    "+ unet_resnet_50_do1_exp_0_192_224_snapshot_50_epochs_qubvel_auto_builder_again\n",
    "best: 0.86148 / 0.90228 / 0.88084\n",
    "snapshot: 0.85951 / 0.89795 / 0.87666\n",
    "    \n",
    "- unet_resnet_50_do1_exp_0_192_224_snapshot_50_epochs_qubvel_auto_builder_add_init_img\n",
    "best: 0.85951 / 0.89791 / 0.87618\n",
    "snapshot: 0.85741 / 0.89639 / 0.87482\n",
    "    \n",
    "- unet_resnet_50_do1_exp_0_192_224_snapshot_50_epochs_qubvel_auto_builder_x2_capacity\n",
    "best: 0.85222 / 0.89566 / 0.87342\n",
    "snapshot: 0.85457 / 0.89454 / 0.87289\n",
    "    \n",
    "- unet_resnet_50_do2_exp_0_192_224_snapshot_50_epochs_drop_init_img\n",
    "best: 0.86358 / 0.89917 / 0.87736\n",
    "snapshot: 0.85975 / 0.8994 / 0.8768\n",
    "    \n",
    "- unet_resnet_50_do3_exp_0_192_224_snapshot_50_epochs_auto_resnet_152\n",
    "best: 0.85778 / 0.8966 / 0.87498\n",
    "snapshot: 0.86123 / 0.89748 / 0.87638\n",
    "\n",
    "!\n",
    "+. just luck?! unet_resnet_50_do1_exp_0_192_224_snapshot_50_epochs_qubvel_auto_builder_old_total_replicate\n",
    "best: 0.86852 / 0.90052 / 0.8795\n",
    "snapshot: 0.85864 / 0.89404 / 0.87322\n",
    "\n",
    "http://neural.profitero.local:8888/edit/babakhin/Branding/salt_old/kaggle-salt/models/segmentation_models/segmentation_models/unet/blocks.py\n",
    "correct batchnorm in unet_resnet_50_do1_exp_0_192_224_snapshot_50_epochs_qubvel_auto_builder_v1         \n",
    "unet_resnet_50_do1_exp_0_192_224_snapshot_50_epochs_qubvel_auto_builder_fix_bn\n",
    "best: 0.85765 / 0.8986 / 0.87597\n",
    "snapshot: 0.85753 / 0.89853 / 0.87647\n",
    "\n",
    "+++ unet_resnet_50_do1_exp_0_192_224_snapshot_50_epochs_qubvel_auto_builder_finetune_lovash\n",
    "best: 0.87148 / 0.90725 / 0.88548\n",
    "snapshot: 0.87222 / 0.90683 / 0.88511\n",
    "    \n",
    "unet_resnet_152_192_224_snapshot_100_epochs_bs_16_finetune_lovash_v1    \n",
    "best: 0.87654 / 0.9079 / 0.88781\n",
    "snapshot: 0.87654 / 0.9079 / 0.88781"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_resnet_50_exp_0_192_224_snapshot_50_epochs\n",
    "best: 0.85914 / 0.89893 / 0.87727 +\n",
    "snapshot: 0.85864 / 0.89403 / 0.87285 -\n",
    "\n",
    "unet_resnet_50_exp_0_192_224_snapshot_50_epochs_vp_again\n",
    "best: 0.8579 / 0.89637 / 0.87465\n",
    "snapshot: 0.85642 / 0.89566 / 0.87397\n",
    "    \n",
    "unet_resnet_50_exp_0_192_224_snapshot_50_epochs_vp_1\n",
    "best: 0.85827 / 0.89388 / 0.87227\n",
    "snapshot: 0.85605 / 0.89327 / 0.87145\n",
    "    \n",
    "unet_resnet_50_exp_0_192_224_snapshot_50_epochs_vp_2\n",
    "best: 0.85753 / 0.89551 / 0.8735\n",
    "snapshot: 0.85605 / 0.8934 / 0.87187\n",
    "    \n",
    "\n",
    "unet_resnet_50_exp_0_192_224_snapshot_50_epochs_vp_3\n",
    "fold_0.hdf5\n",
    "0.85284 / 0.89302 / 0.87159\n",
    "snapshot-1.h5\n",
    "0.85975 / 0.89489 / 0.87413\n",
    "\n",
    "unet_resnet_50_exp_0_192_224_snapshot_50_epochs_vp_4\n",
    "fold_0.hdf5\n",
    "0.84938 / 0.88963 / 0.86796\n",
    "snapshot-1.h5\n",
    "0.85469 / 0.8939 / 0.87209\n",
    "\n",
    "unet_resnet_50_exp_0_192_224_snapshot_50_epochs_vp_5\n",
    "fold_0.hdf5\n",
    "0.84136 / 0.87235 / 0.85144\n",
    "snapshot-1.h5\n",
    "0.85728 / 0.88928 / 0.86796\n",
    "    \n",
    "best: \n",
    "snapshot: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_resnet_50_exp_0_128_128_snapshot_50_epochs\n",
    "best: 0.84 / 0.88259 / 0.85977 + \n",
    "snapshot: 0.84012 / 0.88206 / 0.85958\n",
    "    \n",
    "unet_resnet_50_exp_0_160_160_snapshot_50_epochs\n",
    "best: 0.85815 / 0.89563 / 0.8748 + \n",
    "snapshot: 0.84951 / 0.88725 / 0.8658\n",
    "    \n",
    "unet_resnet_50_exp_0_192_192_snapshot_50_epochs\n",
    "best: 0.84938 / 0.88747 / 0.86692 +\n",
    "snapshot: 0.84605 / 0.8844 / 0.86327\n",
    "    \n",
    "unet_resnet_50_exp_0_224_224_snapshot_50_epochs\n",
    "best: 0.84568 / 0.8805 / 0.85981 +\n",
    "snapshot: 0.84753 / 0.88444 / 0.86321 +\n",
    "    \n",
    "unet_resnet_50_exp_0_256_256_snapshot_50_epochs\n",
    "best: 0.83395 / 0.87394 / 0.8526 +\n",
    "snapshot: 0.83037 / 0.86933 / 0.84849 +\n",
    "    \n",
    "unet_resnet_50_exp_0_144_192_snapshot_50_epochs\n",
    "best: 0.85827 / 0.89389 / 0.87291 +\n",
    "snapshot: 0.85457 / 0.89015 / 0.86881 +\n",
    "    \n",
    "unet_resnet_50_exp_0_160_192_snapshot_50_epochs\n",
    "best: 0.85568 / 0.89174 / 0.86991 +\n",
    "snapshot: 0.85494 / 0.89127 / 0.86885 +\n",
    "    \n",
    "unet_resnet_50_exp_0_176_192_snapshot_50_epochs\n",
    "best: 0.85815 / 0.89138 / 0.87119 +\n",
    "snapshot: 0.85025 / 0.88615 / 0.86518 -\n",
    "    \n",
    "unet_resnet_50_exp_0_144_224_snapshot_50_epochs\n",
    "best: 0.85198 / 0.88523 / 0.8643 +\n",
    "snapshot: 0.85494 / 0.89099 / 0.86941 -\n",
    "    \n",
    "unet_resnet_50_exp_0_192_224_snapshot_50_epochs\n",
    "best: 0.85914 / 0.89893 / 0.87727 +\n",
    "snapshot: 0.85864 / 0.89403 / 0.87285 -\n",
    "    \n",
    "unet_resnet_50_exp_0_202_256_snapshot_50_epochs\n",
    "best: 0.84901 / 0.88852 / 0.86643 +\n",
    "snapshot: 0.85136 / 0.8927 / 0.87084 -\n",
    "    \n",
    "unet_resnet_152_exp_0_192_224_snapshot_50_epochs\n",
    "best: 0.86395 / 0.89585 / 0.8749 +\n",
    "snapshot: 0.86321 / 0.89516 / 0.87405 -\n",
    "    \n",
    "unet_resnet_50_exp_0_144_192_snapshot_50_epochs_lr_00002_v2\n",
    "best: 0.85519 / 0.88991 / 0.86846\n",
    "snapshot: 0.85815 / 0.89012 / 0.86858\n",
    "\n",
    "    \n",
    "0.87284 +"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
